--- old/src/hotspot/cpu/x86/assembler_x86.cpp	2018-05-18 11:35:42.480398493 +0100
+++ new/src/hotspot/cpu/x86/assembler_x86.cpp	2018-05-18 11:35:42.237397252 +0100
@@ -8249,6 +8249,34 @@
   emit_operand(rdi, adr);
 }
 
+void Assembler::clflushopt(Address adr) {
+  // adr should be base reg only with no index or offset
+  assert(adr.index() == noreg, "index should be noreg");
+  assert(adr.scale() == Address::no_scale, "scale should be no_scale");
+  assert(adr.disp() == 0, "displacement should be 0");
+  // prefix is 0x66
+  emit_int8(0x66);
+  // opcode family is 0x0f 0xAE
+  emit_int8(0x0F);
+  emit_int8((unsigned char)0xAE);
+  // extended opcode byte is 7 == rdi
+  emit_operand(rdi, adr);
+}
+
+void Assembler::clwb(Address adr) {
+  // adr should be base reg only with no index or offset
+  assert(adr.index() == noreg, "index should be noreg");
+  assert(adr.scale() == Address::no_scale, "scale should be no_scale");
+  assert(adr.disp() == 0, "displacement should be 0");
+  // prefix is 0x66
+  emit_int8(0x66);
+  // opcode family is 0x0f 0xAE
+  emit_int8(0x0F);
+  emit_int8((unsigned char)0xAE);
+  // extended opcode byte is 6 == rsi
+  emit_operand(rsi, adr);
+}
+
 void Assembler::cmovq(Condition cc, Register dst, Register src) {
   int encode = prefixq_and_encode(dst->encoding(), src->encoding());
   emit_int8(0x0F);
--- old/src/hotspot/cpu/x86/assembler_x86.hpp	2018-05-18 11:35:43.086401589 +0100
+++ new/src/hotspot/cpu/x86/assembler_x86.hpp	2018-05-18 11:35:42.843400347 +0100
@@ -982,6 +982,8 @@
   void cld();
 
   void clflush(Address adr);
+  void clflushopt(Address adr);
+  void clwb(Address adr);
 
   void cmovl(Condition cc, Register dst, Register src);
   void cmovl(Condition cc, Register dst, Address src);
--- old/src/hotspot/cpu/x86/macroAssembler_x86.cpp	2018-05-18 11:35:43.621404322 +0100
+++ new/src/hotspot/cpu/x86/macroAssembler_x86.cpp	2018-05-18 11:35:43.367403024 +0100
@@ -10931,6 +10931,45 @@
   bind(done);
 }
 
+void MacroAssembler::cache_sync(Register addr, Register len, Register tmp)
+{
+  // 64 bit cpus always support clflush
+  assert(VM_Version::supports_clflush(), "should not reach here on 32-bit");
+  bool optimized = VM_Version::supports_clflushopt();
+  bool no_evict = VM_Version::supports_clwb();
+  Label loop, done;
+  Address line(addr, 0);
+  int mask = ~(DEFAULT_CACHE_LINE_SIZE - 1);
+
+  // end addr is addr + ;len
+  movptr(tmp, addr);
+  addptr(tmp, len);
+  // round addr down to beginning of cache line
+  andptr(addr, mask);
+  // uncomment to insert a gdb breakpoint
+  // emit_int8(0xCC);
+  bind(loop);
+  cmpq(addr, tmp);
+  jcc(Assembler::greaterEqual, done);
+  if (optimized) {
+    if (no_evict) {
+      clwb(line);
+    } else {
+      clflushopt(line);
+    }
+  } else {
+    // no need for fence when using CLFLUSH
+    clflush(line);
+  }    
+  addptr(addr, DEFAULT_CACHE_LINE_SIZE);
+  jmp(loop);
+  bind(done);
+  if (optimized || no_evict) {
+    // should be sfence()
+    mfence();
+  }
+}
+
 Assembler::Condition MacroAssembler::negate_condition(Assembler::Condition cond) {
   switch (cond) {
     // Note some conditions are synonyms for others
--- old/src/hotspot/cpu/x86/macroAssembler_x86.hpp	2018-05-18 11:35:44.276407668 +0100
+++ new/src/hotspot/cpu/x86/macroAssembler_x86.hpp	2018-05-18 11:35:44.031406416 +0100
@@ -1729,6 +1729,8 @@
   void byte_array_inflate(Register src, Register dst, Register len,
                           XMMRegister tmp1, Register tmp2);
 
+  // writeback an address range to (persistent) storage
+  void cache_sync(Register addr, Register len, Register tmp);
 };
 
 /**
--- old/src/hotspot/cpu/x86/vm_version_x86.hpp	2018-05-18 11:35:44.913410922 +0100
+++ new/src/hotspot/cpu/x86/vm_version_x86.hpp	2018-05-18 11:35:44.596409303 +0100
@@ -218,7 +218,10 @@
                avx512dq : 1,
                         : 1,
                     adx : 1,
-                        : 6,
+                        : 3,
+             clflushopt : 1,
+                   clwb : 1,
+                        : 1,
                avx512pf : 1,
                avx512er : 1,
                avx512cd : 1,
@@ -335,7 +338,9 @@
 #define CPU_VZEROUPPER ((uint64_t)UCONST64(0x1000000000))       // Vzeroupper instruction
 #define CPU_AVX512_VPOPCNTDQ ((uint64_t)UCONST64(0x2000000000)) // Vector popcount
 #define CPU_VPCLMULQDQ ((uint64_t)UCONST64(0x4000000000)) //Vector carryless multiplication
-
+#define CPU_FLUSHOPT ((uint64_t)UCONST64(0x8000000000)) // flushopt instruction
+#define CPU_CLWB ((uint64_t)UCONST64(0x10000000000)) // clwb instruction
+ 
   enum Extended_Family {
     // AMD
     CPU_FAMILY_AMD_11H       = 0x11,
@@ -569,6 +574,8 @@
       result |= CPU_SHA;
     if (_cpuid_info.std_cpuid1_ecx.bits.fma != 0)
       result |= CPU_FMA;
+    if (_cpuid_info.sef_cpuid7_ebx.bits.clflushopt != 0)
+      result |= CPU_FLUSHOPT;
 
     // AMD features.
     if (is_amd()) {
@@ -588,6 +595,9 @@
       if (_cpuid_info.ext_cpuid1_ecx.bits.misalignsse != 0) {
         result |= CPU_3DNOW_PREFETCH;
       }
+      if (_cpuid_info.sef_cpuid7_ebx.bits.clflushopt != 0) {
+        result |= CPU_CLWB;
+      }
     }
 
     // ZX features.
@@ -921,6 +931,31 @@
   // that can be used for efficient implementation of
   // the intrinsic for java.lang.Thread.onSpinWait()
   static bool supports_on_spin_wait() { return supports_sse2(); }
+
+  // there are several insns to force cache line sync to memory which
+  // we can use to ensure mapped persistent memory is up todate with
+  // pending in-cache changes.
+  //
+  // 64 bit cpus always support clflush which writes back and evicts
+  //
+  // clflushopt is optional and acts like clflush except it does
+  // not synchronize with other memory ops. it needs a preceding
+  // and trailing fence
+  //
+  // clwb is optional writes back without evicting the line. it also
+  // does not synchronize with other memory ops. so, it also needs a
+  // preceding and trailing fence.
+  
+#ifdef _LP64
+  static bool supports_clflush() { return true; }
+  static bool supports_clflushopt() { return (_features & CPU_FLUSHOPT == 0); }
+  static bool supports_clwb() { return (_features & CPU_CLWB == 0); }
+#else
+  static bool supports_clflush() { return true; }
+  static bool supports_clflushopt() { return false; }
+  static bool supports_clwb() { return false; }
+#endif // _LP64
+  
 };
 
 #endif // CPU_X86_VM_VM_VERSION_X86_HPP
--- old/src/hotspot/cpu/x86/x86_64.ad	2018-05-18 11:35:45.501413926 +0100
+++ new/src/hotspot/cpu/x86/x86_64.ad	2018-05-18 11:35:45.225412516 +0100
@@ -6092,6 +6092,21 @@
   ins_pipe(pipe_slow); // XXX
 %}
 
+instruct cacheSync(memory addr, rRegL len, rRegL tmp, rFlagsReg cr)
+%{
+  match(CacheSync addr len);
+
+  effect(USE len, TEMP tmp, KILL cr);
+  ins_cost(100);
+  format %{"cache sync $addr, $len" %}
+  ins_encode %{
+    assert($addr->index_position() < 0, "should be");
+    assert($addr$$disp == 0, "should be");
+    __ cache_sync($addr$$base$$Register, $len$$Register, $tmp$$Register);
+  %}
+  ins_pipe(pipe_slow); // XXX
+%}
+
 //----------BSWAP Instructions-------------------------------------------------
 instruct bytes_reverse_int(rRegI dst) %{
   match(Set dst (ReverseBytesI dst));
--- old/src/hotspot/share/adlc/formssel.cpp	2018-05-18 11:35:46.122417099 +0100
+++ new/src/hotspot/share/adlc/formssel.cpp	2018-05-18 11:35:45.874415831 +0100
@@ -3513,6 +3513,8 @@
   int cnt = sizeof(needs_ideal_memory_list)/sizeof(char*);
   if( strcmp(_opType,"PrefetchAllocation")==0 )
     return 1;
+  if( strcmp(_opType,"CacheSync")==0 )
+    return 1;
   if( _lChild ) {
     const char *opType = _lChild->_opType;
     for( int i=0; i<cnt; i++ )
--- old/src/hotspot/share/classfile/vmSymbols.hpp	2018-05-18 11:35:46.674419918 +0100
+++ new/src/hotspot/share/classfile/vmSymbols.hpp	2018-05-18 11:35:46.406418549 +0100
@@ -1347,6 +1347,10 @@
    do_name(     getAndSetObject_name,                                   "getAndSetObject")                                    \
    do_signature(getAndSetObject_signature,                              "(Ljava/lang/Object;JLjava/lang/Object;)Ljava/lang/Object;" ) \
                                                                                                                                \
+  do_class(java_nio_MappedByteBuffer,                                    "java/nio/MappedByteBuffer")                          \
+   do_name(     force0_mapsync_name,                                    "force0_mapsync")                                        \
+  do_intrinsic(_force0_mapsync, java_nio_MappedByteBuffer, force0_mapsync_name, long_long_void_signature, F_RN)  \
+                                                                                                                               \
    /* (2) Bytecode intrinsics                                                                        */                        \
                                                                                                                                \
   do_intrinsic(_park,                     jdk_internal_misc_Unsafe,     park_name, park_signature,                     F_R)    \
@@ -1532,7 +1536,7 @@
     #undef VM_INTRINSIC_ENUM
 
     ID_LIMIT,
-    LAST_COMPILER_INLINE = _getAndSetObject,
+    LAST_COMPILER_INLINE = _force0_mapsync,
     FIRST_MH_SIG_POLY    = _invokeGeneric,
     FIRST_MH_STATIC      = _linkToVirtual,
     LAST_MH_SIG_POLY     = _linkToInterface,
--- old/src/hotspot/share/opto/c2compiler.cpp	2018-05-18 11:35:47.306423147 +0100
+++ new/src/hotspot/share/opto/c2compiler.cpp	2018-05-18 11:35:47.046421819 +0100
@@ -594,6 +594,10 @@
   case vmIntrinsics::_isCompileConstant:
   case vmIntrinsics::_Preconditions_checkIndex:
     break;
+  case vmIntrinsics::_force0_mapsync:
+    if (getenv("NO_FORCE_MAPSYNC_INTRINSIC")) { return false; }
+    if (!Matcher::match_rule_supported(Op_CacheSync)) return false;
+    break;
   default:
     return false;
   }
--- old/src/hotspot/share/opto/classes.hpp	2018-05-18 11:35:47.836425855 +0100
+++ new/src/hotspot/share/opto/classes.hpp	2018-05-18 11:35:47.587424583 +0100
@@ -48,6 +48,7 @@
 macro(ReverseBytesUS)
 macro(ReverseBytesS)
 macro(CProj)
+macro(CacheSync)
 macro(CallDynamicJava)
 macro(CallJava)
 macro(CallLeaf)
--- old/src/hotspot/share/opto/library_call.cpp	2018-05-18 11:35:48.396428716 +0100
+++ new/src/hotspot/share/opto/library_call.cpp	2018-05-18 11:35:48.141427413 +0100
@@ -326,6 +326,7 @@
   bool inline_montgomerySquare();
   bool inline_vectorizedMismatch();
   bool inline_fma(vmIntrinsics::ID id);
+  bool inline_native_MappedByteBuffer_force0_mapsync();
 
   bool inline_profileBoolean();
   bool inline_isCompileConstant();
@@ -870,6 +871,9 @@
   case vmIntrinsics::_fmaF:
     return inline_fma(intrinsic_id());
 
+  case vmIntrinsics::_force0_mapsync:
+    return inline_native_MappedByteBuffer_force0_mapsync();
+    
   default:
     // If you get here, it may be that someone has added a new intrinsic
     // to the list in vmSymbols.hpp without implementing it here.
@@ -6999,3 +7003,21 @@
   set_result(n->is_Con() ? intcon(1) : intcon(0));
   return true;
 }
+
+//-------------inline_native_MappedByteBuffer_force0_mapsync-----------------------------------
+bool LibraryCallKit::inline_native_MappedByteBuffer_force0_mapsync() {
+#ifndef PRODUCT
+  ciSignature* sig = callee()->signature();
+  assert(sig->type_at(0)->basic_type() == T_LONG, "force0_mapsync address is long");
+  assert(sig->type_at(1)->basic_type() == T_LONG, "force0_mapsync length is long");
+#endif
+  null_check_receiver();  // null-check, then ignore
+  Node *addr = argument(1);
+  addr = new CastX2PNode(addr);
+  addr = _gvn.transform(addr);
+  Node *len = argument(3);
+  Node *flush = new CacheSyncNode(control(), memory(TypeRawPtr::BOTTOM), addr, len);
+  flush = _gvn.transform(flush);
+  set_memory(flush, TypeRawPtr::BOTTOM);
+  return true;
+}
--- old/src/hotspot/share/opto/memnode.hpp	2018-05-18 11:35:49.012431862 +0100
+++ new/src/hotspot/share/opto/memnode.hpp	2018-05-18 11:35:48.740430473 +0100
@@ -1605,4 +1605,15 @@
   virtual const Type *bottom_type() const { return ( AllocatePrefetchStyle == 3 ) ? Type::MEMORY : Type::ABIO; }
 };
 
+// cache sync node for guaranteeing writeback of an address range to
+// (persistent) storage
+class CacheSyncNode : public Node {
+public:
+  CacheSyncNode(Node *ctrl, Node *mem, Node *addr, Node *len) : Node(ctrl, mem, addr, len) {}
+  virtual int Opcode() const;
+  virtual uint ideal_reg() const { return NotAMachineReg; }
+  virtual uint match_edge(uint idx) const { return (idx == 2 || idx == 3); }
+  virtual const Type *bottom_type() const { return Type::MEMORY; }
+};
+
 #endif // SHARE_VM_OPTO_MEMNODE_HPP
--- old/src/java.base/share/classes/java/nio/Direct-X-Buffer.java.template	2018-05-18 11:35:49.556434641 +0100
+++ new/src/java.base/share/classes/java/nio/Direct-X-Buffer.java.template	2018-05-18 11:35:49.302433344 +0100
@@ -167,15 +167,15 @@
     //
     protected Direct$Type$Buffer$RW$(int cap, long addr,
                                      FileDescriptor fd,
-                                     Runnable unmapper)
+                                     Runnable unmapper, boolean isMapSync)
     {
 #if[rw]
-        super(-1, 0, cap, cap, fd);
+        super(-1, 0, cap, cap, fd, isMapSync);
         address = addr;
         cleaner = Cleaner.create(this, unmapper);
         att = null;
 #else[rw]
-        super(cap, addr, fd, unmapper);
+        super(cap, addr, fd, unmapper, isMapSync);
         this.isReadOnly = true;
 #end[rw]
     }
--- old/src/java.base/share/classes/java/nio/MappedByteBuffer.java	2018-05-18 11:35:50.089437364 +0100
+++ new/src/java.base/share/classes/java/nio/MappedByteBuffer.java	2018-05-18 11:35:49.816435970 +0100
@@ -28,7 +28,7 @@
 import java.io.FileDescriptor;
 import java.lang.ref.Reference;
 import jdk.internal.misc.Unsafe;
-
+import jdk.internal.HotSpotIntrinsicCandidate;
 
 /**
  * A direct byte buffer whose content is a memory-mapped region of a file.
@@ -67,6 +67,10 @@
 public abstract class MappedByteBuffer
     extends ByteBuffer
 {
+    private static native void registerNatives();
+    static {
+        registerNatives();
+    }
 
     // This is a little bit backwards: By rights MappedByteBuffer should be a
     // subclass of DirectByteBuffer, but to keep the spec clear and simple, and
@@ -77,18 +81,30 @@
     // operations if valid; null if the buffer is not mapped.
     private final FileDescriptor fd;
 
+    private final boolean isMapSync;
+
     // This should only be invoked by the DirectByteBuffer constructors
     //
     MappedByteBuffer(int mark, int pos, int lim, int cap, // package-private
+                     FileDescriptor fd, boolean isMapSync)
+    {
+        super(mark, pos, lim, cap);
+        this.fd = fd;
+        this.isMapSync = isMapSync;
+    }
+
+    MappedByteBuffer(int mark, int pos, int lim, int cap, // package-private
                      FileDescriptor fd)
     {
         super(mark, pos, lim, cap);
         this.fd = fd;
+        this.isMapSync = false;
     }
 
     MappedByteBuffer(int mark, int pos, int lim, int cap) { // package-private
         super(mark, pos, lim, cap);
         this.fd = null;
+        this.isMapSync = false;
     }
 
     // Returns the distance (in bytes) of the buffer from the page aligned address
@@ -108,6 +124,15 @@
     }
 
     /**
+     * Returns the memory mapping type of this buffer.
+     *
+     * @return true if mapped with MAP_SYNC, otherwise false.
+     */
+    public boolean isMapSync() {
+        return isMapSync;
+    }
+
+    /**
      * Tells whether or not this buffer's content is resident in physical
      * memory.
      *
@@ -202,12 +227,55 @@
      * @return  This buffer
      */
     public final MappedByteBuffer force() {
+        return force(0, capacity());
+    }
+
+    /**
+     * Forces any changes made to this buffer's content to be written to the
+     * storage device containing the mapped file.
+     *
+     * <p> If the file mapped into this buffer resides on a local storage
+     * device then when this method returns it is guaranteed that all changes
+     * made to the buffer since it was created, or since this method was last
+     * invoked, will have been written to that device.
+     *
+     * <p> If the file does not reside on a local device then no such guarantee
+     * is made.
+     *
+     * <p> If this buffer was not mapped in read/write mode ({@link
+     * java.nio.channels.FileChannel.MapMode#READ_WRITE}) then invoking this
+     * method has no effect. </p>
+     *
+     * @param from the index of the start of the range to force
+     * @param to the index of the end of the range to force (inclusive)
+     *
+     * @return  This buffer
+     */
+    public final MappedByteBuffer force(long from, long to) {
         if (fd == null) {
             return this;
         }
         if ((address != 0) && (capacity() != 0)) {
+
+            if(from < 0 || from > capacity()) {
+                throw new IllegalArgumentException();
+            }
+            if(to < from || to < capacity()) {
+                throw new IllegalArgumentException();
+            }
+
             long offset = mappingOffset();
-            force0(fd, mappingAddress(offset), mappingLength(offset));
+            long a = mappingAddress(offset) + from;
+            long length = offset + to;
+
+            if(isMapSync) {
+                // fastpath avoiding the syscall (and JNI)
+                // System.out.println("fastpath"); // TODO remove me
+                force0_mapsync(a, length);
+            } else {
+                // System.out.println("slowpath"); // TODO remove me
+                force0(fd, a, length);
+            }
         }
         return this;
     }
@@ -215,6 +283,8 @@
     private native boolean isLoaded0(long address, long length, int pageCount);
     private native void load0(long address, long length);
     private native void force0(FileDescriptor fd, long address, long length);
+    @HotSpotIntrinsicCandidate
+    private native void force0_mapsync(long address, long length);
 
     // -- Covariant return type overrides
 
--- old/src/java.base/share/classes/sun/nio/ch/FileChannelImpl.java	2018-05-18 11:35:50.595439949 +0100
+++ new/src/java.base/share/classes/sun/nio/ch/FileChannelImpl.java	2018-05-18 11:35:50.343438662 +0100
@@ -932,7 +932,11 @@
     private static final int MAP_RW = 1;
     private static final int MAP_PV = 2;
 
-    public MappedByteBuffer map(MapMode mode, long position, long size)
+    public MappedByteBuffer map(MapMode mode, long position, long size) throws IOException {
+        return map(mode, position, size, false);
+    }
+
+    public MappedByteBuffer map(MapMode mode, long position, long size, boolean mapSync)
         throws IOException
     {
         ensureOpen();
@@ -996,9 +1000,9 @@
                     // a valid file descriptor is not required
                     FileDescriptor dummy = new FileDescriptor();
                     if ((!writable) || (imode == MAP_RO))
-                        return Util.newMappedByteBufferR(0, 0, dummy, null);
+                        return Util.newMappedByteBufferR(0, 0, dummy, null, mapSync);
                     else
-                        return Util.newMappedByteBuffer(0, 0, dummy, null);
+                        return Util.newMappedByteBuffer(0, 0, dummy, null, mapSync);
                 }
 
                 pagePosition = (int)(position % allocationGranularity);
@@ -1006,7 +1010,7 @@
                 mapSize = size + pagePosition;
                 try {
                     // If map0 did not throw an exception, the address is valid
-                    addr = map0(imode, mapPosition, mapSize);
+                    addr = map0(imode, mapPosition, mapSize, mapSync ? 1 : 0);
                 } catch (OutOfMemoryError x) {
                     // An OutOfMemoryError may indicate that we've exhausted
                     // memory so force gc and re-attempt map
@@ -1017,7 +1021,7 @@
                         Thread.currentThread().interrupt();
                     }
                     try {
-                        addr = map0(imode, mapPosition, mapSize);
+                        addr = map0(imode, mapPosition, mapSize, mapSync ? 1 : 0);
                     } catch (OutOfMemoryError y) {
                         // After a second OOME, fail
                         throw new IOException("Map failed", y);
@@ -1043,12 +1047,14 @@
                 return Util.newMappedByteBufferR(isize,
                                                  addr + pagePosition,
                                                  mfd,
-                                                 um);
+                                                 um,
+                                                 mapSync);
             } else {
                 return Util.newMappedByteBuffer(isize,
                                                 addr + pagePosition,
                                                 mfd,
-                                                um);
+                                                um,
+                                                mapSync);
             }
         } finally {
             threads.remove(ti);
@@ -1202,7 +1208,7 @@
     // -- Native methods --
 
     // Creates a new mapping
-    private native long map0(int prot, long position, long length)
+    private native long map0(int prot, long position, long length, int map_sync)
         throws IOException;
 
     // Removes an existing mapping
--- old/src/java.base/share/classes/sun/nio/ch/Util.java	2018-05-18 11:35:51.138442723 +0100
+++ new/src/java.base/share/classes/sun/nio/ch/Util.java	2018-05-18 11:35:50.879441400 +0100
@@ -408,7 +408,8 @@
                             new Class<?>[] { int.class,
                                              long.class,
                                              FileDescriptor.class,
-                                             Runnable.class });
+                                             Runnable.class,
+                                             boolean.class });
                         ctor.setAccessible(true);
                         directByteBufferConstructor = ctor;
                     } catch (ClassNotFoundException   |
@@ -423,7 +424,8 @@
 
     static MappedByteBuffer newMappedByteBuffer(int size, long addr,
                                                 FileDescriptor fd,
-                                                Runnable unmapper)
+                                                Runnable unmapper,
+                                                boolean isMapSync)
     {
         MappedByteBuffer dbb;
         if (directByteBufferConstructor == null)
@@ -433,7 +435,8 @@
               new Object[] { size,
                              addr,
                              fd,
-                             unmapper });
+                             unmapper,
+                             isMapSync });
         } catch (InstantiationException |
                  IllegalAccessException |
                  InvocationTargetException e) {
@@ -453,7 +456,8 @@
                             new Class<?>[] { int.class,
                                              long.class,
                                              FileDescriptor.class,
-                                             Runnable.class });
+                                             Runnable.class,
+                                             boolean.class });
                         ctor.setAccessible(true);
                         directByteBufferRConstructor = ctor;
                     } catch (ClassNotFoundException |
@@ -468,7 +472,8 @@
 
     static MappedByteBuffer newMappedByteBufferR(int size, long addr,
                                                  FileDescriptor fd,
-                                                 Runnable unmapper)
+                                                 Runnable unmapper,
+                                                 boolean isMapSync)
     {
         MappedByteBuffer dbb;
         if (directByteBufferRConstructor == null)
@@ -478,7 +483,8 @@
               new Object[] { size,
                              addr,
                              fd,
-                             unmapper });
+                             unmapper,
+                             isMapSync });
         } catch (InstantiationException |
                  IllegalAccessException |
                  InvocationTargetException e) {
--- old/src/java.base/unix/native/libnio/MappedByteBuffer.c	2018-05-18 11:35:51.677445477 +0100
+++ new/src/java.base/unix/native/libnio/MappedByteBuffer.c	2018-05-18 11:35:51.432444225 +0100
@@ -33,6 +33,8 @@
 #include <stddef.h>
 #include <stdlib.h>
 
+#include "pmem/jdk_pmem.h"
+
 #ifdef _AIX
 #include <unistd.h>
 #endif
@@ -54,6 +56,34 @@
 }
 #endif
 
+/*
+ * registered native method called instead of force0 when a buffer is
+ * mapped to persistent main memory with MAP_SYNC and MAP_SHARED.
+ *
+ * this uses the libpmem library code to write affected cache lines
+ * back to main memory. if the compiler intrinsic is enabled then the
+ * native call is translated to inline cache writeback instructions.
+ * currently JIT support is only available on linux x86_64.
+ */
+void force0_mapsync(JNIEnv *env, jclass ofClass, jlong address, jlong len) {
+    void* a = (void *)jlong_to_ptr(address);
+    size_t st = (size_t)len;
+    LOG(3, "mappedByteBuffer.force_mapsync addr %p len %li", a, len);
+    pmem_persist(a, st);
+}
+
+static JNINativeMethod methods[] = {
+    {"force0_mapsync",         "(JJ)V",          (void *)&force0_mapsync},
+};
+
+JNIEXPORT void JNICALL
+Java_java_nio_MappedByteBuffer_registerNatives(JNIEnv *env, jclass cls)
+{
+    (*env)->RegisterNatives(env, cls, methods,
+                            sizeof(methods)/sizeof(JNINativeMethod));
+}
+
+
 JNIEXPORT jboolean JNICALL
 Java_java_nio_MappedByteBuffer_isLoaded0(JNIEnv *env, jobject obj, jlong address,
                                          jlong len, jint numPages)
@@ -120,8 +150,10 @@
                                       jlong address, jlong len)
 {
     void* a = (void *)jlong_to_ptr(address);
+    LOG(3, "mappedByteBuffer.force0 addr %p len %li\n", a, len);
     int result = msync(a, (size_t)len, MS_SYNC);
     if (result == -1) {
         JNU_ThrowIOExceptionWithLastError(env, "msync failed");
     }
 }
+
--- old/src/java.base/unix/native/libnio/ch/FileChannelImpl.c	2018-05-18 11:35:52.180448046 +0100
+++ new/src/java.base/unix/native/libnio/ch/FileChannelImpl.c	2018-05-18 11:35:51.935446795 +0100
@@ -47,6 +47,8 @@
 #include "nio_util.h"
 #include "sun_nio_ch_FileChannelImpl.h"
 #include "java_lang_Integer.h"
+// include pmem headers for LOG tracing
+#include "pmem/jdk_pmem.h"
 
 static jfieldID chan_fd;        /* jobject 'fd' in sun.nio.ch.FileChannelImpl */
 
@@ -70,9 +72,17 @@
 }
 
 
+/*
+ * modified FileChannelImpl.map0() takes extra boolean arg map_sync
+ *
+ * setting map_sync requests that persistent main memory be mapped to
+ * back a byte buffer. if set flags MAP_SYNC and MAP_SHARED_VALIDATE
+ * are passed to mmap. leaving map_sync unset requests the standard
+ * mapping behaviour.
+ */
 JNIEXPORT jlong JNICALL
 Java_sun_nio_ch_FileChannelImpl_map0(JNIEnv *env, jobject this,
-                                     jint prot, jlong off, jlong len)
+                                     jint prot, jlong off, jlong len, jint map_sync)
 {
     void *mapAddress = 0;
     jobject fdo = (*env)->GetObjectField(env, this, chan_fd);
@@ -91,6 +101,31 @@
         flags = MAP_PRIVATE;
     }
 
+    if(map_sync) {
+#if defined(__linux__)
+        if(prot == sun_nio_ch_FileChannelImpl_MAP_PV) {
+            JNU_ThrowIOException(env, "mmap with MAP_SYNC and MAP_PV not allowed");
+            return IOS_THROWN;
+        }
+
+        /*
+        /usr/include/asm-generic/mman.h:#define MAP_SYNC	0x80000 // perform synchronous page faults for the mapping
+        /usr/include/asm-generic/mman-common.h:#define MAP_SHARED_VALIDATE 0x03	// share + validate extension flags
+        */
+        int MAP_SYNC = 0x80000;
+        int MAP_SHARED_VALIDATE = 0x03;
+        flags = flags | MAP_SYNC | MAP_SHARED_VALIDATE;
+#else
+        // TODO - implement for solaris/AIX/BSD/WINDOWS
+        JNU_ThrowIOException(env, "mmap with MAP_SYNC not supported");
+        return IOS_THROWN;
+#endif
+}
+
+    if (map_sync) {
+        LOG(3, "FileChannelImpl.map0 flags %i\n", flags);
+    }
+
     mapAddress = mmap64(
         0,                    /* Let OS decide location */
         len,                  /* Number of bytes to map */
@@ -104,9 +139,19 @@
             JNU_ThrowOutOfMemoryError(env, "Map failed");
             return IOS_THROWN;
         }
+
+        if(map_sync && errno == ENOTSUP) {
+            JNU_ThrowIOExceptionWithLastError(env, "MAP_SYNC not supported");
+            return IOS_THROWN;
+        }
+
         return handle(env, -1, "Map failed");
     }
 
+    if (map_sync) {
+        LOG(3, "FileChannelImpl.map0 addr %p\n", mapAddress);
+    }
+
     return ((jlong) (unsigned long) mapAddress);
 }
 
--- /dev/null	2018-05-10 08:46:08.277910066 +0100
+++ new/src/java.base/unix/native/libnio/pmem/cpu.c	2018-05-18 11:35:52.435449349 +0100
@@ -0,0 +1,180 @@
+/*
+ * Copyright 2015-2017, Intel Corporation
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *
+ *     * Neither the name of the copyright holder nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/*
+ * cpu.c -- CPU features detection
+ *
+ * These routines do not work AARCH64 platforms, and need new detection
+ * routiones to be added. Currently to ensure msync is not used and ARM
+ * FLUSH instructions are used PMEM_IS_PMEM_FORCE=1 needs to be used.
+ */
+
+/*
+ * Reference:
+ * http://www.intel.com/content/www/us/en/processors/
+ * architectures-software-developer-manuals.html
+ *
+ * https://support.amd.com/TechDocs/24594.pdf
+ */
+
+#include <string.h>
+#include <stdlib.h>
+
+#include "jdk_pmem.h"
+#include "cpu.h"
+
+#define EAX_IDX 0
+#define EBX_IDX 1
+#define ECX_IDX 2
+#define EDX_IDX 3
+
+#if defined(__x86_64__) || defined(__amd64__)
+
+#include <cpuid.h>
+
+static inline void
+cpuid(unsigned func, unsigned subfunc, unsigned cpuinfo[4])
+{
+	__cpuid_count(func, subfunc, cpuinfo[EAX_IDX], cpuinfo[EBX_IDX],
+			cpuinfo[ECX_IDX], cpuinfo[EDX_IDX]);
+}
+
+#elif defined(_M_X64) || defined(_M_AMD64)
+
+#include <intrin.h>
+
+static inline void
+cpuid(unsigned func, unsigned subfunc, unsigned cpuinfo[4])
+{
+	__cpuidex(cpuinfo, func, subfunc);
+}
+
+#else /* not x86_64 */
+
+#define cpuid(func, subfunc, cpuinfo)\
+	do { (void)(func); (void)(subfunc); (void)(cpuinfo); } while (0)
+
+#endif
+
+#ifndef bit_CLFLUSH
+#define bit_CLFLUSH	(1 << 19)
+#endif
+
+#ifndef bit_CLFLUSHOPT
+#define bit_CLFLUSHOPT	(1 << 23)
+#endif
+
+#ifndef bit_CLWB
+#define bit_CLWB	(1 << 24)
+#endif
+
+/*
+ * is_cpu_feature_present -- (internal) checks if CPU feature is supported
+ */
+static int
+is_cpu_feature_present(unsigned func, unsigned reg, unsigned bit)
+{
+	unsigned cpuinfo[4] = { 0 };
+
+	/* check CPUID level first */
+	cpuid(0x0, 0x0, cpuinfo);
+	if (cpuinfo[EAX_IDX] < func)
+		return 0;
+
+	cpuid(func, 0x0, cpuinfo);
+	return (cpuinfo[reg] & bit) != 0;
+}
+
+/*
+ * is_cpu_genuine_intel -- checks for genuine Intel CPU
+ */
+int
+is_cpu_genuine_intel(void)
+{
+	unsigned cpuinfo[4] = { 0 };
+
+	union {
+		char name[0x20];
+		unsigned cpuinfo[3];
+	} vendor;
+
+	memset(&vendor, 0, sizeof(vendor));
+
+	cpuid(0x0, 0x0, cpuinfo);
+
+	vendor.cpuinfo[0] = cpuinfo[EBX_IDX];
+	vendor.cpuinfo[1] = cpuinfo[EDX_IDX];
+	vendor.cpuinfo[2] = cpuinfo[ECX_IDX];
+
+	LOG(4, "CPU vendor: %s", vendor.name);
+	return (strncmp(vendor.name, "GenuineIntel",
+				sizeof(vendor.name))) == 0;
+}
+
+/*
+ * is_cpu_clflush_present -- checks if CLFLUSH instruction is supported
+ */
+int
+is_cpu_clflush_present(void)
+{
+	int ret = is_cpu_feature_present(0x1, EDX_IDX, bit_CLFLUSH);
+	LOG(4, "CLFLUSH %ssupported", ret == 0 ? "not " : "");
+
+	return ret;
+}
+
+/*
+ * is_cpu_clflushopt_present -- checks if CLFLUSHOPT instruction is supported
+ */
+int
+is_cpu_clflushopt_present(void)
+{
+	int ret = is_cpu_feature_present(0x7, EBX_IDX, bit_CLFLUSHOPT);
+	LOG(4, "CLFLUSHOPT %ssupported", ret == 0 ? "not " : "");
+
+	return ret;
+}
+
+/*
+ * is_cpu_clwb_present -- checks if CLWB instruction is supported
+ */
+int
+is_cpu_clwb_present(void)
+{
+	if (!is_cpu_genuine_intel())
+		return 0;
+
+	int ret = is_cpu_feature_present(0x7, EBX_IDX, bit_CLWB);
+	LOG(4, "CLWB %ssupported", ret == 0 ? "not " : "");
+
+	return ret;
+}
--- /dev/null	2018-05-10 08:46:08.277910066 +0100
+++ new/src/java.base/unix/native/libnio/pmem/cpu.h	2018-05-18 11:35:52.948451970 +0100
@@ -0,0 +1,45 @@
+/*
+ * Copyright 2016-2017, Intel Corporation
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *
+ *     * Neither the name of the copyright holder nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef PMDK_CPU_H
+#define PMDK_CPU_H 1
+
+/*
+ * cpu.h -- definitions for "cpu" module
+ */
+
+int is_cpu_genuine_intel(void);
+int is_cpu_clflush_present(void);
+int is_cpu_clflushopt_present(void);
+int is_cpu_clwb_present(void);
+
+#endif
--- /dev/null	2018-05-10 08:46:08.277910066 +0100
+++ new/src/java.base/unix/native/libnio/pmem/jdk_pmem.h	2018-05-18 11:35:53.454454555 +0100
@@ -0,0 +1,52 @@
+
+void pmem_init(void);
+
+void pmem_persist(const void *addr, size_t len);
+
+#ifndef _MSC_VER
+#define ATTR_CONSTRUCTOR __attribute__((constructor)) static
+#define ATTR_DESTRUCTOR __attribute__((destructor)) static
+#else
+#define ATTR_CONSTRUCTOR
+#define ATTR_DESTRUCTOR
+#endif
+
+// util.h
+#define util_bool_compare_and_swap32 __sync_bool_compare_and_swap
+#define util_fetch_and_add32 __sync_fetch_and_add
+
+// out.h
+
+#define EVALUATE_DBG_EXPRESSIONS 0
+
+#define FORMAT_PRINTF(a, b) __attribute__((__format__(__printf__, (a), (b))))
+
+void out_log(const char *file, int line, const char *func, int level,
+	const char *fmt, ...) FORMAT_PRINTF(5, 6);
+
+#define OUT_LOG out_log
+
+/* produce debug/trace output */
+#define LOG(level, ...) do { \
+	if (!EVALUATE_DBG_EXPRESSIONS) break;\
+	OUT_LOG(__FILE__, __LINE__, __func__, level, __VA_ARGS__);\
+} while (0)
+
+#define OUT_FATAL_ABORT out_fatal_abort
+
+static __attribute__((always_inline)) __attribute__((noreturn)) inline void
+out_fatal_abort(const char *file, int line, const char *func,
+		const char *fmt, ...)
+{
+	(void) file;
+	(void) line;
+	(void) func;
+	(void) fmt;
+
+	abort();
+}
+
+/* produce output and exit */
+#define FATAL(...)\
+	OUT_FATAL_ABORT(__FILE__, __LINE__, __func__, __VA_ARGS__)
+
--- /dev/null	2018-05-10 08:46:08.277910066 +0100
+++ new/src/java.base/unix/native/libnio/pmem/out.c	2018-05-18 11:35:53.953457104 +0100
@@ -0,0 +1,17 @@
+#include <stdarg.h>
+#include <stdio.h>
+
+void
+out_log(const char *file, int line, const char *func, int level,
+		const char *fmt, ...)
+{
+	printf("jdk_pmem_logging: %s %i %s ", file, line, func);
+
+	va_list ap;
+	va_start(ap, fmt);
+	vprintf(fmt, ap);
+	va_end(ap);
+
+	printf("\n");
+
+}
--- /dev/null	2018-05-10 08:46:08.277910066 +0100
+++ new/src/java.base/unix/native/libnio/pmem/pmem.c	2018-05-18 11:35:54.452459653 +0100
@@ -0,0 +1,639 @@
+/*
+ * Copyright 2014-2018, Intel Corporation
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *
+ *     * Neither the name of the copyright holder nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/*
+ * pmem.c -- pmem entry points for libpmem
+ *
+ *
+ * PERSISTENT MEMORY INSTRUCTIONS ON X86
+ *
+ * The primary feature of this library is to provide a way to flush
+ * changes to persistent memory as outlined below (note that many
+ * of the decisions below are made at initialization time, and not
+ * repeated every time a flush is requested).
+ *
+ * To flush a range to pmem when CLWB is available:
+ *
+ *	CLWB for each cache line in the given range.
+ *
+ *	SFENCE to ensure the CLWBs above have completed.
+ *
+ * To flush a range to pmem when CLFLUSHOPT is available and CLWB is not
+ * (same as above but issue CLFLUSHOPT instead of CLWB):
+ *
+ *	CLFLUSHOPT for each cache line in the given range.
+ *
+ *	SFENCE to ensure the CLWBs above have completed.
+ *
+ * To flush a range to pmem when neither CLFLUSHOPT or CLWB are available
+ * (same as above but fences surrounding CLFLUSH are not required):
+ *
+ *	CLFLUSH for each cache line in the given range.
+ *
+ * To memcpy a range of memory to pmem when MOVNT is available:
+ *
+ *	Copy any non-64-byte portion of the destination using MOV.
+ *
+ *	Use the flush flow above without the fence for the copied portion.
+ *
+ *	Copy using MOVNTDQ, up to any non-64-byte aligned end portion.
+ *	(The MOVNT instructions bypass the cache, so no flush is required.)
+ *
+ *	Copy any unaligned end portion using MOV.
+ *
+ *	Use the flush flow above for the copied portion (including fence).
+ *
+ * To memcpy a range of memory to pmem when MOVNT is not available:
+ *
+ *	Just pass the call to the normal memcpy() followed by pmem_persist().
+ *
+ * To memset a non-trivial sized range of memory to pmem:
+ *
+ *	Same as the memcpy cases above but store the given value instead
+ *	of reading values from the source.
+ *
+ * These features are supported for ARM AARCH64 using equivalent ARM
+ * assembly instruction. Please refer to (arm_cacheops.h) for more details.
+ *
+ * INTERFACES FOR FLUSHING TO PERSISTENT MEMORY
+ *
+ * Given the flows above, three interfaces are provided for flushing a range
+ * so that the caller has the ability to separate the steps when necessary,
+ * but otherwise leaves the detection of available instructions to the libpmem:
+ *
+ * pmem_persist(addr, len)
+ *
+ *	This is the common case, which just calls the two other functions:
+ *
+ *		pmem_flush(addr, len);
+ *		pmem_drain();
+ *
+ * pmem_flush(addr, len)
+ *
+ *	CLWB or CLFLUSHOPT or CLFLUSH for each cache line
+ *
+ * pmem_drain()
+ *
+ *	SFENCE unless using CLFLUSH
+ *
+ *
+ * INTERFACES FOR COPYING/SETTING RANGES OF MEMORY
+ *
+ * Given the flows above, the following interfaces are provided for the
+ * memmove/memcpy/memset operations to persistent memory:
+ *
+ * pmem_memmove_nodrain()
+ *
+ *	Checks for overlapped ranges to determine whether to copy from
+ *	the beginning of the range or from the end.  If MOVNT instructions
+ *	are available, uses the memory copy flow described above, otherwise
+ *	calls the libc memmove() followed by pmem_flush(). Since no conditional
+ *	compilation and/or architecture specific CFLAGS are in use at the
+ *	moment, SSE2 ( thus movnt ) is just assumed to be available.
+ *
+ * pmem_memcpy_nodrain()
+ *
+ *	Just calls pmem_memmove_nodrain().
+ *
+ * pmem_memset_nodrain()
+ *
+ *	If MOVNT instructions are available, uses the memset flow described
+ *	above, otherwise calls the libc memset() followed by pmem_flush().
+ *
+ * pmem_memmove_persist()
+ * pmem_memcpy_persist()
+ * pmem_memset_persist()
+ *
+ *	Calls the appropriate _nodrain() function followed by pmem_drain().
+ *
+ *
+ * DECISIONS MADE AT INITIALIZATION TIME
+ *
+ * As much as possible, all decisions described above are made at library
+ * initialization time.  This is achieved using function pointers that are
+ * setup by pmem_init() when the library loads.
+ *
+ *	Func_predrain_fence is used by pmem_drain() to call one of:
+ *		predrain_fence_empty()
+ *		predrain_memory_barrier()
+ *
+ *	Func_flush is used by pmem_flush() to call one of:
+ *		flush_dcache()
+ *		flush_dcache_invalidate_opt()
+ *		flush_dcache_invalidate()
+ *
+ *	Func_memmove_nodrain is used by memmove_nodrain() to call one of:
+ *		memmove_nodrain_normal()
+ *		memmove_nodrain_movnt()
+ *
+ *	Func_memset_nodrain is used by memset_nodrain() to call one of:
+ *		memset_nodrain_normal()
+ *		memset_nodrain_movnt()
+ *
+ * DEBUG LOGGING
+ *
+ * Many of the functions here get called hundreds of times from loops
+ * iterating over ranges, making the usual LOG() calls at level 3
+ * impractical.  The call tracing log for those functions is set at 15.
+ */
+
+#ifndef _GNU_SOURCE
+#define _GNU_SOURCE
+#endif
+
+#include <sys/mman.h>
+#include <sys/stat.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <stdint.h>
+#include <string.h>
+#ifndef __aarch64__
+#include <emmintrin.h>
+#endif
+#include <errno.h>
+#include <fcntl.h>
+#include <unistd.h>
+#include <limits.h>
+
+#ifdef _WIN32
+#include <memoryapi.h>
+#endif
+
+#include "cpu.h"
+#include "jdk_pmem.h"
+
+
+/*
+ * copy from libpmem.c
+ *
+ * libpmem_init -- load-time initialization for libpmem
+ *
+ * Called automatically by the run-time loader.
+ */
+ATTR_CONSTRUCTOR
+void
+libpmem_init(void)
+{
+	LOG(3, NULL);
+	pmem_init();
+}
+
+
+
+// copy from os_posix.c. requires #def _GNU_SOURCE
+char *
+os_getenv(const char *name)
+{
+	return secure_getenv(name);
+}
+
+
+#ifndef _MSC_VER
+/*
+ * The x86 memory instructions are new enough that the compiler
+ * intrinsic functions are not always available.  The intrinsic
+ * functions are defined here in terms of asm statements for now.
+ */
+
+#ifndef __aarch64__
+#define _mm_clflushopt(addr)\
+	asm volatile(".byte 0x66; clflush %0" : "+m" (*(volatile char *)addr));
+#define _mm_clwb(addr)\
+	asm volatile(".byte 0x66; xsaveopt %0" : "+m" (*(volatile char *)addr));
+#endif /* __aarch64__ */
+#endif /* _MSC_VER */
+
+#define FLUSH_ALIGN ((uintptr_t)64)
+
+#ifndef __aarch64__
+#define ALIGN_MASK	(FLUSH_ALIGN - 1)
+
+#define CHUNK_SIZE	128 /* 16*8 */
+#define CHUNK_SHIFT	7
+#define CHUNK_MASK	(CHUNK_SIZE - 1)
+
+#define DWORD_SIZE	4
+#define DWORD_SHIFT	2
+#define DWORD_MASK	(DWORD_SIZE - 1)
+
+#define MOVNT_SIZE	16
+#define MOVNT_MASK	(MOVNT_SIZE - 1)
+#define MOVNT_SHIFT	4
+
+#define MOVNT_THRESHOLD	256
+
+static size_t Movnt_threshold = MOVNT_THRESHOLD;
+#endif
+
+/*
+ * pmem_has_hw_drain -- return whether or not HW drain was found
+ *
+ * Always false for x86: HW drain is done by HW with no SW involvement.
+ */
+int
+pmem_has_hw_drain(void)
+{
+	LOG(3, NULL);
+
+	return 0;
+}
+
+/*
+ * predrain_fence_empty -- (internal) issue the pre-drain fence instruction
+ */
+static void
+predrain_fence_empty(void)
+{
+	LOG(15, NULL);
+	/* nothing to do (because CLFLUSH did it for us) */
+}
+
+/*
+ * predrain_memory_barrier -- (internal) issue the pre-drain fence instruction
+ */
+static void
+predrain_memory_barrier(void)
+{
+	LOG(15, NULL);
+#ifndef __aarch64__
+	_mm_sfence();	/* ensure CLWB or CLFLUSHOPT completes */
+#else
+	arm_data_memory_barrier();
+#endif
+}
+
+/*
+ * pmem_drain() calls through Func_predrain_fence to do the fence.  Although
+ * initialized to predrain_fence_empty(), once the existence of the CLWB or
+ * CLFLUSHOPT feature is confirmed by pmem_init() at library initialization
+ * time, Func_predrain_fence is set to predrain_memory_barrier().  That's the
+ * most common case on modern hardware that supports persistent memory.
+ */
+static void (*Func_predrain_fence)(void) = predrain_fence_empty;
+
+/*
+ * pmem_drain -- wait for any PM stores to drain from HW buffers
+ */
+void
+pmem_drain(void)
+{
+	LOG(15, NULL);
+
+	Func_predrain_fence();
+}
+
+
+#ifdef __aarch64__
+/*
+ * flush_dcache does similar to clwb using DC CVAC
+ */
+static void
+flush_dcache(const void *addr, size_t len)
+{
+	LOG(15, "addr %p len %zu", addr, len);
+
+	uintptr_t uptr;
+
+	/*
+	 * Loop through cache-line-size (typically 64B) aligned chunks
+	 * covering the given range.
+	 */
+	for (uptr = (uintptr_t)addr & ~(FLUSH_ALIGN - 1);
+		uptr < (uintptr_t)addr + len; uptr += FLUSH_ALIGN) {
+		arm_clean_va_to_poc((char *)uptr);
+	}
+}
+
+#else
+
+/*
+ * flush_dcache_invalidate -- (internal) flush the CPU cache, using clflush
+ */
+static void
+flush_dcache_invalidate(const void *addr, size_t len)
+{
+	LOG(15, "addr %p len %zu", addr, len);
+
+	uintptr_t uptr;
+
+	/*
+	 * Loop through cache-line-size (typically 64B) aligned chunks
+	 * covering the given range.
+	 */
+	for (uptr = (uintptr_t)addr & ~(FLUSH_ALIGN - 1);
+		uptr < (uintptr_t)addr + len; uptr += FLUSH_ALIGN) {
+		_mm_clflush((char *)uptr);
+	}
+}
+
+/*
+ * flush_dcache -- (internal) flush the CPU cache, using clwb
+ */
+static void
+flush_dcache(const void *addr, size_t len)
+{
+	LOG(15, "addr %p len %zu", addr, len);
+
+	uintptr_t uptr;
+
+	/*
+	 * Loop through cache-line-size (typically 64B) aligned chunks
+	 * covering the given range.
+	 */
+	for (uptr = (uintptr_t)addr & ~(FLUSH_ALIGN - 1);
+		uptr < (uintptr_t)addr + len; uptr += FLUSH_ALIGN) {
+		_mm_clwb((char *)uptr);
+	}
+}
+#endif
+
+/*
+ * flush_dcache_invalidate_opt -- (internal) flush the CPU cache,
+ * using clflushopt for X86 and arm_clean_and_invalidate_va_to_poc
+ * for aarch64 (see arm_cacheops.h) {DC CIVAC}
+ */
+
+#ifdef __aarch64__
+static void
+flush_dcache_invalidate_opt(const void *addr, size_t len)
+{
+	LOG(15, "addr %p len  %zu", addr, len);
+
+	uintptr_t uptr;
+
+	arm_data_memory_barrier();
+	for (uptr = (uintptr_t)addr & ~(FLUSH_ALIGN - 1);
+		uptr < (uintptr_t)addr + len; uptr += FLUSH_ALIGN) {
+		arm_clean_and_invalidate_va_to_poc((char *)uptr);
+	}
+	arm_data_memory_barrier();
+}
+
+#else
+
+static void
+flush_dcache_invalidate_opt(const void *addr, size_t len)
+{
+	LOG(15, "addr %p len %zu", addr, len);
+
+	uintptr_t uptr;
+
+	/*
+	 * Loop through cache-line-size (typically 64B) aligned chunks
+	 * covering the given range.
+	 */
+	for (uptr = (uintptr_t)addr & ~(FLUSH_ALIGN - 1);
+		uptr < (uintptr_t)addr + len; uptr += FLUSH_ALIGN) {
+		_mm_clflushopt((char *)uptr);
+	}
+}
+#endif
+
+/*
+ * flush_empty -- (internal) do not flush the CPU cache
+ */
+static void
+flush_empty(const void *addr, size_t len)
+{
+	LOG(15, "addr %p len %zu", addr, len);
+
+	/* NOP */
+}
+
+/*
+ * pmem_flush() calls through Func_flush to do the work.  Although
+ * initialized to flush_dcache_invalidate(), once the existence of the
+ * clflushopt feature is confirmed by pmem_init() at library
+ * initialization time, Func_flush is set to flush_dcache_invalidate_opt().
+ * That's the most common case on modern hardware that supports persistent
+ * memory. In case of aarch64, there is no difference between clflush and
+ * clflushopt so both refer to flush_data_clean_invalidate.
+ */
+#ifndef __aarch64__
+static void (*Func_deep_flush)(const void *, size_t) = flush_dcache_invalidate;
+static void (*Func_flush)(const void *, size_t) = flush_dcache_invalidate;
+#else
+static void (*Func_deep_flush)(const void *, size_t) =
+						flush_dcache_invalidate_opt;
+static void (*Func_flush)(const void *, size_t) =
+						flush_dcache_invalidate_opt;
+#endif
+
+/*
+ * pmem_deep_flush -- flush processor cache for the given range
+ * regardless of eADR support on platform
+ */
+void
+pmem_deep_flush(const void *addr, size_t len)
+{
+	LOG(15, "addr %p len %zu", addr, len);
+
+	Func_deep_flush(addr, len);
+}
+
+/*
+ * pmem_flush -- flush processor cache for the given range
+ */
+void
+pmem_flush(const void *addr, size_t len)
+{
+	LOG(15, "addr %p len %zu", addr, len);
+
+	Func_flush(addr, len);
+}
+
+/*
+ * pmem_persist -- make any cached changes to a range of pmem persistent
+ */
+void
+pmem_persist(const void *addr, size_t len)
+{
+	LOG(15, "addr %p len %zu", addr, len);
+
+	pmem_flush(addr, len);
+	pmem_drain();
+}
+
+
+
+#define PMEM_FILE_ALL_FLAGS\
+	(PMEM_FILE_CREATE|PMEM_FILE_EXCL|PMEM_FILE_SPARSE|PMEM_FILE_TMPFILE)
+
+#define PMEM_DAX_VALID_FLAGS\
+	(PMEM_FILE_CREATE|PMEM_FILE_SPARSE)
+
+
+/*
+ * pmem_log_cpuinfo -- log the results of cpu dispatching decisions,
+ * and verify them
+ */
+static void
+pmem_log_cpuinfo(void)
+{
+	LOG(3, NULL);
+
+#ifndef __aarch64__
+	if (Func_deep_flush == flush_dcache)
+		LOG(3, "using clwb");
+	else if (Func_deep_flush == flush_dcache_invalidate_opt)
+		LOG(3, "using clflushopt");
+	else if (Func_deep_flush == flush_dcache_invalidate)
+		LOG(3, "using clflush");
+	else
+		FATAL("invalid deep flush function address");
+
+	if (Func_flush == flush_empty)
+		LOG(3, "not flushing CPU cache");
+	else if (Func_flush != Func_deep_flush)
+		FATAL("invalid flush function address");
+
+#else
+	if (Func_deep_flush == flush_dcache)
+		LOG(3, "Using ARM invalidate");
+	else if (Func_deep_flush == flush_dcache_invalidate_opt)
+		LOG(3, "Synchronize VA to poc for ARM");
+	else
+		FATAL("invalid flush function address");
+#endif
+
+
+}
+
+/*
+ * pmem_get_cpuinfo -- configure libpmem based on CPUID
+ */
+static void
+pmem_get_cpuinfo(void)
+{
+	LOG(3, NULL);
+
+	if (is_cpu_clflush_present()) {
+		// Func_is_pmem = is_pmem_detect;
+		LOG(3, "clflush supported");
+	}
+
+	if (is_cpu_clflushopt_present()) {
+		LOG(3, "clflushopt supported");
+
+		char *e = os_getenv("PMEM_NO_CLFLUSHOPT");
+		if (e && strcmp(e, "1") == 0)
+			LOG(3, "PMEM_NO_CLFLUSHOPT forced no clflushopt");
+		else {
+			Func_deep_flush = flush_dcache_invalidate_opt;
+			Func_predrain_fence = predrain_memory_barrier;
+		}
+	}
+
+	if (is_cpu_clwb_present()) {
+		LOG(3, "clwb supported");
+
+		char *e = os_getenv("PMEM_NO_CLWB");
+		if (e && strcmp(e, "1") == 0)
+			LOG(3, "PMEM_NO_CLWB forced no clwb");
+		else {
+			Func_deep_flush = flush_dcache;
+			Func_predrain_fence = predrain_memory_barrier;
+		}
+	}
+}
+
+/*
+ * pmem_init -- load-time initialization for pmem.c
+ */
+void
+pmem_init(void)
+{
+	LOG(3, NULL);
+
+	pmem_get_cpuinfo();
+
+	Func_flush = Func_deep_flush;
+
+	char *e = os_getenv("PMEM_NO_FLUSH");
+	if (e && strcmp(e, "1") == 0) {
+		LOG(3, "forced not flushing CPU cache");
+		Func_flush = flush_empty;
+		Func_predrain_fence = predrain_memory_barrier;
+	}
+
+	/*
+	 * XXX - check if eADR is available
+	 * replace with pmem_auto_flush()
+	 */
+	char *auto_flush = os_getenv("PMEM_NO_FLUSH");
+	if (auto_flush && strcmp(auto_flush, "1") == 0) {
+		LOG(3, "eADR is available");
+		Func_flush = flush_empty;
+		Func_predrain_fence = predrain_memory_barrier;
+	}
+
+/*
+ * non-temporal is currently not supported in ARM so defaulting to
+ * memcpy_nodrain_normal
+ */
+#ifndef __aarch64__
+	/*
+	 * For testing, allow overriding the default threshold
+	 * for using non-temporal stores in pmem_memcpy_*(), pmem_memmove_*()
+	 * and pmem_memset_*().
+	 * It has no effect if movnt is not supported or disabled.
+	 */
+	char *ptr = os_getenv("PMEM_MOVNT_THRESHOLD");
+	if (ptr) {
+		long long val = atoll(ptr);
+
+		if (val < 0)
+			LOG(3, "Invalid PMEM_MOVNT_THRESHOLD");
+		else {
+			LOG(3, "PMEM_MOVNT_THRESHOLD set to %zu", (size_t)val);
+			Movnt_threshold = (size_t)val;
+		}
+	}
+
+	ptr = os_getenv("PMEM_NO_MOVNT");
+	if (ptr && strcmp(ptr, "1") == 0)
+		LOG(3, "PMEM_NO_MOVNT forced no movnt");
+	else {
+		// Func_memmove_nodrain = memmove_nodrain_movnt;
+		// Func_memset_nodrain = memset_nodrain_movnt;
+	}
+#endif
+	pmem_log_cpuinfo();
+
+#if defined(_WIN32) && (NTDDI_VERSION >= NTDDI_WIN10_RS1)
+	Func_qvmi = (PQVM)GetProcAddress(
+			GetModuleHandle(TEXT("KernelBase.dll")),
+			"QueryVirtualMemoryInformation");
+#endif
+}
+
+
